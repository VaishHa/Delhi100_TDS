{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTJorW6m4Uuy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Manually set the environment variable\n",
        "os.environ[\"GITHUB_TOKEN\"] = \"*Place your token here to access API*\"\n",
        "\n",
        "# Confirm it was set\n",
        "print(\"GITHUB_TOKEN:\", os.getenv('GITHUB_TOKEN'))\n",
        "headers = {\n",
        "    \"Authorization\": f\"token {os.getenv('GITHUB_TOKEN')}\",\n",
        "    \"Accept\": \"application/vnd.github.v3+json\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZStJ9jM7Ozz"
      },
      "source": [
        "This is the updated code for the users from delhi with large followers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EZ02c3O7OYp"
      },
      "outputs": [],
      "source": [
        "#This is the code to scrape the users from Delhi with more than 100 followers.\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Authentication and rate limiting\n",
        "token = os.getenv(\"GITHUB_TOKEN\")\n",
        "headers = {\n",
        "    \"Authorization\": f\"token {token}\",\n",
        "    \"Accept\": \"application/vnd.github.v3+json\"\n",
        "}\n",
        "# rate_limit_per_hour = 700  # GitHub API rate limit\n",
        "# sleep_interval = 3600 / rate_limit_per_hour  # Approximate delay per request\n",
        "\n",
        "# Parameters for fetching users\n",
        "location = \"Delhi\"\n",
        "min_followers = 100\n",
        "per_page = 30  # Max items per page in GitHub API\n",
        "max_user_pages = 25  # Control the number of pages for fetching users\n",
        "\n",
        "# Base URL for user search and individual user details\n",
        "search_url = \"https://api.github.com/search/users\"\n",
        "user_detail_url_template = \"https://api.github.com/users/{}\"\n",
        "\n",
        "# List to store user data\n",
        "user_data_list = []\n",
        "\n",
        "for page in range(1, max_user_pages + 1):\n",
        "    # Search users by location and minimum follower count\n",
        "    params = {\n",
        "        \"q\": f\"location:{location} followers:>{min_followers}\",\n",
        "        \"per_page\": per_page,\n",
        "        \"page\": page\n",
        "    }\n",
        "\n",
        "    response = requests.get(search_url, headers=headers, params=params)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "\n",
        "        for user in data[\"items\"]:\n",
        "            username = user[\"login\"]\n",
        "\n",
        "            # Fetch individual user details\n",
        "            user_detail_response = requests.get(user_detail_url_template.format(username), headers=headers)\n",
        "            if user_detail_response.status_code == 200:\n",
        "                user_data = user_detail_response.json()\n",
        "\n",
        "                # Extract the required fields\n",
        "                user_info = {\n",
        "                    \"login\": user_data.get(\"login\"),\n",
        "                    \"name\": user_data.get(\"name\"),\n",
        "                    \"company\": user_data.get(\"company\"),\n",
        "                    \"location\": user_data.get(\"location\"),\n",
        "                    \"email\": user_data.get(\"email\"),\n",
        "                    \"hireable\": user_data.get(\"hireable\"),\n",
        "                    \"bio\": user_data.get(\"bio\"),\n",
        "                    \"public_repos\": user_data.get(\"public_repos\"),\n",
        "                    \"followers\": user_data.get(\"followers\"),\n",
        "                    \"following\": user_data.get(\"following\"),\n",
        "                    \"created_at\": user_data.get(\"created_at\")\n",
        "                }\n",
        "                print(user_info)\n",
        "\n",
        "                # Append to the list\n",
        "                user_data_list.append(user_info)\n",
        "\n",
        "                # Delay to respect rate limits\n",
        "                # time.sleep(sleep_interval)\n",
        "            else:\n",
        "                print(f\"Failed to fetch details for user: {username}\")\n",
        "\n",
        "        # Stop if we get fewer users than `per_page` (indicating last page)\n",
        "        if len(data[\"items\"]) < per_page:\n",
        "            break\n",
        "\n",
        "        # Delay between pages\n",
        "        # time.sleep(2)\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        break\n",
        "\n",
        "# Save data to JSON file\n",
        "with open(\"/content/github_user_delhi.json\", \"w\") as f:\n",
        "    json.dump(user_data_list, f, indent=4)\n",
        "\n",
        "print(\"Data saved to github_user_delhi.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0o7b_-cuLY3"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n",
        "\n",
        "# File paths\n",
        "json_file_path = \"/content/github_user_delhi.json\"\n",
        "csv_file_path = \"/content/github_user_delhi.csv\"\n",
        "\n",
        "# Load JSON data\n",
        "with open(json_file_path, \"r\") as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "# Define the CSV field names (headers)\n",
        "fieldnames = [\"login\", \"name\", \"company\", \"location\", \"email\", \"hireable\", \"bio\",\n",
        "              \"public_repos\", \"followers\", \"following\", \"created_at\"]\n",
        "\n",
        "# Write to CSV file\n",
        "with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    writer.writeheader()  # Write the header row\n",
        "\n",
        "    # Write each user's data row\n",
        "    for user_info in data:\n",
        "        writer.writerow(user_info)\n",
        "\n",
        "print(\"Data successfully converted to github_users_delhi.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuTkFuiJ6Xb0"
      },
      "source": [
        "This is the second part of the project which creates repo.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22GXkkx66bgT"
      },
      "outputs": [],
      "source": [
        "#This code creates the repository.csv by scraping it from Github.\n",
        "import requests\n",
        "import csv\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Authentication and rate limiting\n",
        "token = os.getenv(\"GITHUB_TOKEN\")\n",
        "headers = {\n",
        "    \"Authorization\": f\"token {token}\",\n",
        "    \"Accept\": \"application/vnd.github.v3+json\"\n",
        "}\n",
        "# rate_limit_per_hour = 700  # GitHub API rate limit\n",
        "# sleep_interval = 3600 / rate_limit_per_hour  # Approximate delay per request\n",
        "\n",
        "# Parameters for fetching repositories\n",
        "per_page = 100  # Max items per page in GitHub API\n",
        "max_repos = 500  # Max repositories to fetch per user\n",
        "\n",
        "# Base URL for fetching user repositories\n",
        "repos_url_template = \"https://api.github.com/users/{}/repos\"\n",
        "\n",
        "# File paths\n",
        "users_csv_path = \"/content/github_user_delhi.csv\"\n",
        "repos_csv_path = \"/content/repositories.csv\"\n",
        "\n",
        "# Define the CSV field names for repositories.csv\n",
        "repo_fieldnames = [\n",
        "    \"login\", \"full_name\", \"created_at\", \"stargazers_count\",\n",
        "    \"watchers_count\", \"language\", \"has_projects\", \"has_wiki\", \"license_name\"\n",
        "]\n",
        "\n",
        "# List to store repository data\n",
        "repo_data_list = []\n",
        "\n",
        "# Open users.csv to read usernames and repositories.csv to write repository data\n",
        "with open(users_csv_path, \"r\") as users_file:\n",
        "    users_reader = csv.DictReader(users_file)\n",
        "\n",
        "    # Process each user\n",
        "    for user in users_reader:\n",
        "        username = user[\"login\"]  # The GitHub user ID (login) of the owner\n",
        "        print(f\"Fetching repositories for user: {username}\")\n",
        "\n",
        "        # Fetch repositories for the user\n",
        "        repo_count = 0\n",
        "        for page in range(1, (max_repos // per_page) + 2):\n",
        "            params = {\n",
        "                \"per_page\": per_page,\n",
        "                \"page\": page,\n",
        "                \"sort\": \"pushed\"  # Sort by most recently pushed\n",
        "            }\n",
        "\n",
        "            response = requests.get(repos_url_template.format(username), headers=headers, params=params)\n",
        "            if response.status_code == 200:\n",
        "                repos = response.json()\n",
        "\n",
        "                for repo in repos:\n",
        "                    repo_info = {\n",
        "                        \"login\": repo[\"owner\"][\"login\"],  # User ID of the owner\n",
        "                        \"full_name\": repo.get(\"full_name\"),\n",
        "                        \"created_at\": repo.get(\"created_at\"),\n",
        "                        \"stargazers_count\": repo.get(\"stargazers_count\"),\n",
        "                        \"watchers_count\": repo.get(\"watchers_count\"),\n",
        "                        \"language\": repo.get(\"language\"),\n",
        "                        \"has_projects\": repo.get(\"has_projects\"),\n",
        "                        \"has_wiki\": repo.get(\"has_wiki\"),\n",
        "                        \"license_name\": repo[\"license\"][\"name\"] if repo.get(\"license\") else None\n",
        "                    }\n",
        "\n",
        "                    # Add repository info to the list\n",
        "                    repo_data_list.append(repo_info)\n",
        "\n",
        "                    repo_count += 1\n",
        "                    if repo_count >= max_repos:\n",
        "                        break\n",
        "\n",
        "                # Break out of the loop if fewer than `per_page` results are returned (end of repos)\n",
        "                if len(repos) < per_page or repo_count >= max_repos:\n",
        "                    break\n",
        "\n",
        "                # Delay to respect rate limits\n",
        "                # time.sleep(sleep_interval)\n",
        "            else:\n",
        "                print(f\"Failed to fetch repositories for user: {username}, Status Code: {response.status_code}\")\n",
        "                break\n",
        "\n",
        "# Save repositories data to CSV\n",
        "with open(repos_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as repos_file:\n",
        "    repos_writer = csv.DictWriter(repos_file, fieldnames=repo_fieldnames)\n",
        "    repos_writer.writeheader()\n",
        "    repos_writer.writerows(repo_data_list)\n",
        "\n",
        "print(\"Repository data saved to repositories.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtIpRhvhJTnR"
      },
      "source": [
        "This is sorted by created at"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afQt4RCpJTTx"
      },
      "outputs": [],
      "source": [
        "#This sorts the users.csv by created_at column and saves it in new file.\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "# File path for users.csv\n",
        "users_csv_path = \"/content/users.csv\"\n",
        "sorted_users_csv_path = \"/content/sorted_users.csv\"\n",
        "\n",
        "# Read users from users.csv\n",
        "with open(users_csv_path, mode=\"r\", encoding=\"utf-8\") as users_file:\n",
        "    users_reader = csv.DictReader(users_file)\n",
        "    users_list = list(users_reader)  # Convert to a list for sorting\n",
        "\n",
        "# Sort users by created_at (in ascending order)\n",
        "sorted_users_list = sorted(users_list, key=lambda x: datetime.strptime(x['created_at'], '%Y-%m-%dT%H:%M:%SZ'))\n",
        "\n",
        "# Write sorted users to a new CSV file\n",
        "with open(sorted_users_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as sorted_users_file:\n",
        "    fieldnames = users_reader.fieldnames  # Use the same field names\n",
        "    sorted_users_writer = csv.DictWriter(sorted_users_file, fieldnames=fieldnames)\n",
        "    sorted_users_writer.writeheader()\n",
        "    sorted_users_writer.writerows(sorted_users_list)\n",
        "\n",
        "print(f\"Users sorted by 'created_at' and saved to {sorted_users_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSaO-mLOMCGh"
      },
      "outputs": [],
      "source": [
        "#This finds the top three licenses used.\n",
        "import csv\n",
        "from collections import Counter\n",
        "\n",
        "# File path for repositories.csv\n",
        "repos_csv_path = \"/content/repositories.csv\"\n",
        "\n",
        "# Initialize a counter for licenses\n",
        "license_counter = Counter()\n",
        "\n",
        "# Read repositories from repositories.csv\n",
        "with open(repos_csv_path, mode=\"r\", encoding=\"utf-8\") as repos_file:\n",
        "    repos_reader = csv.DictReader(repos_file)\n",
        "\n",
        "    # Count licenses while ignoring missing values\n",
        "    for repo in repos_reader:\n",
        "        license_name = repo.get(\"license_name\")\n",
        "        if license_name:  # Ignore missing licenses\n",
        "            license_counter[license_name] += 1\n",
        "\n",
        "# Get the three most common licenses\n",
        "most_common_licenses = license_counter.most_common(3)\n",
        "\n",
        "# Display the results\n",
        "print(\"Three Most Popular Licenses:\")\n",
        "for license_name, count in most_common_licenses:\n",
        "    print(f\"{license_name}: {count} occurrences\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMVRjSkUMs5B"
      },
      "source": [
        "Cleaning Company Name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKdc7sj8MsnS"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# File paths for users.csv and cleaned_users.csv\n",
        "users_csv_path = \"/content/users.csv\"\n",
        "cleaned_users_csv_path = \"/content/users.csv\"\n",
        "\n",
        "# Read users from users.csv and clean up company names\n",
        "with open(users_csv_path, mode=\"r\", encoding=\"utf-8\") as users_file:\n",
        "    users_reader = csv.DictReader(users_file)\n",
        "\n",
        "    # Prepare a list to hold cleaned user data\n",
        "    cleaned_users_list = []\n",
        "\n",
        "    for user in users_reader:\n",
        "        # Clean the company name\n",
        "        company = user.get(\"company\", \"\").strip()  # Get company and trim whitespace\n",
        "        if company.startswith('@'):\n",
        "            company = company[1:]  # Strip leading @ symbol\n",
        "        company = company.upper()  # Convert to uppercase\n",
        "\n",
        "        # Update the user's company name\n",
        "        user[\"company\"] = company\n",
        "\n",
        "        # Add the cleaned user data to the list\n",
        "        cleaned_users_list.append(user)\n",
        "\n",
        "# Write the cleaned user data to a new CSV file\n",
        "with open(cleaned_users_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as cleaned_users_file:\n",
        "    fieldnames = users_reader.fieldnames  # Use the same field names\n",
        "    cleaned_users_writer = csv.DictWriter(cleaned_users_file, fieldnames=fieldnames)\n",
        "    cleaned_users_writer.writeheader()\n",
        "    cleaned_users_writer.writerows(cleaned_users_list)\n",
        "\n",
        "print(f\"Cleaned company names saved to {cleaned_users_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K68m0uBc8PX5"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# File path for users.csv\n",
        "input_csv_path = \"/content/users.csv\"\n",
        "output_csv_path = \"/content/users.csv\"\n",
        "\n",
        "# Step 1: Read and process users.csv\n",
        "processed_users = []\n",
        "\n",
        "with open(input_csv_path, mode=\"r\", encoding=\"utf-8\") as users_file:\n",
        "    users_reader = csv.DictReader(users_file)\n",
        "\n",
        "    for user in users_reader:\n",
        "        # Convert hireable values: \"True\" to \"true\", \"False\" to \"false\", and None or empty strings as \"null\"\n",
        "        if user.get(\"hireable\") == \"True\":\n",
        "            user[\"hireable\"] = \"true\"\n",
        "        elif user.get(\"hireable\") == \"False\":\n",
        "            user[\"hireable\"] = \"false\"\n",
        "        elif user.get(\"hireable\") in [\"\", None]:\n",
        "            user[\"hireable\"] = \"null\"\n",
        "\n",
        "        # Append modified user data to processed_users list\n",
        "        processed_users.append(user)\n",
        "\n",
        "# Step 2: Write the processed data back to a new CSV file\n",
        "with open(output_csv_path, mode=\"w\", newline='', encoding=\"utf-8\") as output_file:\n",
        "    fieldnames = processed_users[0].keys()\n",
        "    writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "    writer.writerows(processed_users)\n",
        "\n",
        "print(f\"Processed data saved to {output_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB3VBawt8rjY"
      },
      "source": [
        "The users.csv is completely cleaned here above. Now in this step repo.csv is cleaned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-R3P7f0Y8xf3"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# File path for repositories.csv\n",
        "input_csv_path = \"/content/repositories.csv\"\n",
        "output_csv_path = \"/content/repositories.csv\"\n",
        "\n",
        "# Step 1: Read and process repositories.csv\n",
        "processed_repos = []\n",
        "\n",
        "with open(input_csv_path, mode=\"r\", encoding=\"utf-8\") as repos_file:\n",
        "    repos_reader = csv.DictReader(repos_file)\n",
        "\n",
        "    for repo in repos_reader:\n",
        "        # Convert has_projects values: \"True\" to \"true\", \"False\" to \"false\", and None or empty strings as \"null\"\n",
        "        if repo.get(\"has_projects\") == \"True\":\n",
        "            repo[\"has_projects\"] = \"true\"\n",
        "        elif repo.get(\"has_projects\") == \"False\":\n",
        "            repo[\"has_projects\"] = \"false\"\n",
        "        elif repo.get(\"has_projects\") in [\"\", None]:\n",
        "            repo[\"has_projects\"] = \"null\"\n",
        "\n",
        "        # Convert has_wiki values: \"True\" to \"true\", \"False\" to \"false\", and None or empty strings as \"null\"\n",
        "        if repo.get(\"has_wiki\") == \"True\":\n",
        "            repo[\"has_wiki\"] = \"true\"\n",
        "        elif repo.get(\"has_wiki\") == \"False\":\n",
        "            repo[\"has_wiki\"] = \"false\"\n",
        "        elif repo.get(\"has_wiki\") in [\"\", None]:\n",
        "            repo[\"has_wiki\"] = \"null\"\n",
        "\n",
        "        # Append modified repo data to processed_repos list\n",
        "        processed_repos.append(repo)\n",
        "\n",
        "# Step 2: Write the processed data back to a new CSV file\n",
        "with open(output_csv_path, mode=\"w\", newline='', encoding=\"utf-8\") as output_file:\n",
        "    fieldnames = processed_repos[0].keys()\n",
        "    writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "    writer.writerows(processed_repos)\n",
        "\n",
        "print(f\"Processed data saved to {output_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhmioQsrNEKr"
      },
      "outputs": [],
      "source": [
        "#This code finds the company from which maximum developers belong.\n",
        "import csv\n",
        "from collections import Counter\n",
        "\n",
        "# File path for cleaned_users.csv\n",
        "cleaned_users_csv_path = \"/content/users.csv\"\n",
        "\n",
        "# Initialize a counter for companies\n",
        "company_counter = Counter()\n",
        "\n",
        "# Read users from cleaned_users.csv\n",
        "with open(cleaned_users_csv_path, mode=\"r\", encoding=\"utf-8\") as cleaned_users_file:\n",
        "    users_reader = csv.DictReader(cleaned_users_file)\n",
        "\n",
        "    # Count occurrences of each company\n",
        "    for user in users_reader:\n",
        "        company = user.get(\"company\")\n",
        "        if company:  # Ignore missing company names\n",
        "            company_counter[company] += 1\n",
        "\n",
        "# Find the company with the majority of developers\n",
        "most_common_company, most_common_count = company_counter.most_common(1)[0]\n",
        "\n",
        "# Display the result\n",
        "print(f\"The majority of developers work at: {most_common_company} with {most_common_count} developers.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XckwD6NwNZT7"
      },
      "outputs": [],
      "source": [
        "#This finds the most used Programming Language by devs.\n",
        "import csv\n",
        "from collections import Counter\n",
        "\n",
        "# File path for repositories.csv\n",
        "repos_csv_path = \"/content/repositories.csv\"\n",
        "\n",
        "# Initialize a counter for programming languages\n",
        "language_counter = Counter()\n",
        "\n",
        "# Read repositories from repositories.csv\n",
        "with open(repos_csv_path, mode=\"r\", encoding=\"utf-8\") as repos_file:\n",
        "    repos_reader = csv.DictReader(repos_file)\n",
        "\n",
        "    # Count occurrences of each programming language\n",
        "    for repo in repos_reader:\n",
        "        language = repo.get(\"language\")\n",
        "        if language:  # Ignore missing languages\n",
        "            language_counter[language] += 1\n",
        "\n",
        "# Find the most popular programming language\n",
        "most_common_language, most_common_count = language_counter.most_common(1)[0]\n",
        "\n",
        "# Display the result\n",
        "print(f\"The most popular programming language is: {most_common_language} with {most_common_count} repositories.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5cmy6UqNoVr"
      },
      "outputs": [],
      "source": [
        "#This finds the most used Programming Language by devs who joined after 2020.\n",
        "import csv\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "\n",
        "# File paths for users.csv and repositories.csv\n",
        "users_csv_path = \"/content/users.csv\"\n",
        "repos_csv_path = \"/content/repositories.csv\"\n",
        "\n",
        "# Set the threshold date for filtering users\n",
        "threshold_date = datetime(2020, 1, 1)\n",
        "\n",
        "# Step 1: Read users from users.csv and filter based on created_at\n",
        "new_users = set()  # Set to store the login names of users who joined after 2020\n",
        "\n",
        "with open(users_csv_path, mode=\"r\", encoding=\"utf-8\") as users_file:\n",
        "    users_reader = csv.DictReader(users_file)\n",
        "\n",
        "    for user in users_reader:\n",
        "        created_at = datetime.strptime(user[\"created_at\"], '%Y-%m-%dT%H:%M:%SZ')\n",
        "        if created_at > threshold_date:\n",
        "            new_users.add(user[\"login\"])  # Store the login names of filtered users\n",
        "\n",
        "# Step 2: Count programming languages for the filtered users in repositories.csv\n",
        "language_counter = Counter()\n",
        "\n",
        "with open(repos_csv_path, mode=\"r\", encoding=\"utf-8\") as repos_file:\n",
        "    repos_reader = csv.DictReader(repos_file)\n",
        "\n",
        "    for repo in repos_reader:\n",
        "        if repo[\"login\"] in new_users:  # Check if the repository belongs to a new user\n",
        "            language = repo.get(\"language\")\n",
        "            if language:  # Ignore missing languages\n",
        "                language_counter[language] += 1\n",
        "\n",
        "# Step 3: Find the second most popular programming language\n",
        "if len(language_counter) >= 2:\n",
        "    # Get the two most common languages\n",
        "    most_common_languages = language_counter.most_common(2)\n",
        "    second_most_common_language, second_most_common_count = most_common_languages[1]\n",
        "    print(f\"The second most popular programming language among users who joined after 2020 is: \"\n",
        "          f\"{second_most_common_language} with {second_most_common_count} repositories.\")\n",
        "else:\n",
        "    print(\"Not enough data to determine the second most popular programming language.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gLZWwXgN1x2"
      },
      "outputs": [],
      "source": [
        "#This finds the highest average number of stars per repository.\n",
        "import csv\n",
        "from collections import defaultdict\n",
        "\n",
        "# File path for repositories.csv\n",
        "repos_csv_path = \"/content/repositories.csv\"\n",
        "\n",
        "# Initialize dictionaries to hold total stars and counts of repositories per language\n",
        "language_stars = defaultdict(int)  # Total stars per language\n",
        "language_counts = defaultdict(int)  # Count of repositories per language\n",
        "\n",
        "# Step 1: Read repositories from repositories.csv and calculate stars and counts\n",
        "with open(repos_csv_path, mode=\"r\", encoding=\"utf-8\") as repos_file:\n",
        "    repos_reader = csv.DictReader(repos_file)\n",
        "\n",
        "    for repo in repos_reader:\n",
        "        language = repo.get(\"language\")\n",
        "        stargazers_count = repo.get(\"stargazers_count\", 0)\n",
        "\n",
        "        if language and stargazers_count.isdigit():  # Ignore missing languages and non-digit star counts\n",
        "            language_stars[language] += int(stargazers_count)\n",
        "            language_counts[language] += 1\n",
        "\n",
        "# Step 2: Calculate average stars per repository for each language\n",
        "language_averages = {}\n",
        "for language in language_stars:\n",
        "    count = language_counts[language]\n",
        "    if count > 0:\n",
        "        average_stars = language_stars[language] / count\n",
        "        language_averages[language] = average_stars\n",
        "\n",
        "# Step 3: Find the language with the highest average stars per repository\n",
        "if language_averages:\n",
        "    highest_avg_language = max(language_averages, key=language_averages.get)\n",
        "    highest_avg_stars = language_averages[highest_avg_language]\n",
        "\n",
        "    print(f\"The language with the highest average number of stars per repository is: \"\n",
        "          f\"{highest_avg_language} with an average of {highest_avg_stars:.2f} stars.\")\n",
        "else:\n",
        "    print(\"No data available to determine the average stars per repository.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5804ZXAZOEgC"
      },
      "outputs": [],
      "source": [
        "#Top 5 users by leader strengths.\n",
        "import csv\n",
        "\n",
        "# File path for users.csv\n",
        "users_csv_path = \"/content/users.csv\"\n",
        "\n",
        "# Initialize a list to store user data with leader strength\n",
        "leader_strengths = []\n",
        "\n",
        "# Step 1: Read users from users.csv and calculate leader_strength\n",
        "with open(users_csv_path, mode=\"r\", encoding=\"utf-8\") as users_file:\n",
        "    users_reader = csv.DictReader(users_file)\n",
        "\n",
        "    for user in users_reader:\n",
        "        login = user.get(\"login\")\n",
        "        followers = int(user.get(\"followers\", 0))\n",
        "        following = int(user.get(\"following\", 0))\n",
        "\n",
        "        # Calculate leader_strength\n",
        "        leader_strength = followers / (1 + following) if following >= 0 else followers\n",
        "\n",
        "        # Append login and leader strength to the list\n",
        "        leader_strengths.append((login, leader_strength))\n",
        "\n",
        "# Step 2: Sort users by leader_strength in descending order\n",
        "top_users = sorted(leader_strengths, key=lambda x: x[1], reverse=True)[:5]\n",
        "\n",
        "# Step 3: Get the logins of the top 5 users\n",
        "top_logins = [user[0] for user in top_users]\n",
        "\n",
        "# Print the result as a comma-separated string\n",
        "print(\"Top 5 users by leader strength (logins):\", \", \".join(top_logins))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkzI8nEYOVJt"
      },
      "outputs": [],
      "source": [
        "#Correlation between Number of followers and Number of public repositories.\n",
        "import csv\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# File path for users.csv\n",
        "users_csv_path = \"/content/users.csv\"\n",
        "\n",
        "# Initialize lists to store followers and public repository counts\n",
        "followers = []\n",
        "public_repos = []\n",
        "\n",
        "# Step 1: Read users from users.csv and extract followers and public_repos\n",
        "with open(users_csv_path, mode=\"r\", encoding=\"utf-8\") as users_file:\n",
        "    users_reader = csv.DictReader(users_file)\n",
        "\n",
        "    for user in users_reader:\n",
        "        followers_count = int(user.get(\"followers\", 0))\n",
        "        public_repos_count = int(user.get(\"public_repos\", 0))\n",
        "\n",
        "        followers.append(followers_count)\n",
        "        public_repos.append(public_repos_count)\n",
        "\n",
        "# Step 2: Calculate the correlation between followers and public repositories\n",
        "if len(followers) > 1 and len(public_repos) > 1:  # Ensure there is enough data\n",
        "    correlation, _ = pearsonr(followers, public_repos)\n",
        "    print(f\"The correlation between the number of followers and the number of public repositories is: {correlation:.4f}\")\n",
        "else:\n",
        "    print(\"Not enough data to calculate correlation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQ8by5tTOkH-"
      },
      "outputs": [],
      "source": [
        "#Regression Slope of the followers and number of repositories.\n",
        "import csv\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# File path for users.csv\n",
        "users_csv_path = \"/content/users.csv\"\n",
        "\n",
        "# Initialize lists to store followers and public repository counts\n",
        "followers = []\n",
        "public_repos = []\n",
        "\n",
        "# Step 1: Read users from users.csv and extract followers and public_repos\n",
        "with open(users_csv_path, mode=\"r\", encoding=\"utf-8\") as users_file:\n",
        "    users_reader = csv.DictReader(users_file)\n",
        "\n",
        "    for user in users_reader:\n",
        "        followers_count = int(user.get(\"followers\", 0))\n",
        "        public_repos_count = int(user.get(\"public_repos\", 0))\n",
        "\n",
        "        followers.append(followers_count)\n",
        "        public_repos.append(public_repos_count)\n",
        "\n",
        "# Step 2: Prepare the data for regression analysis\n",
        "X = np.array(public_repos)  # Independent variable (public repositories)\n",
        "y = np.array(followers)      # Dependent variable (followers)\n",
        "\n",
        "# Add a constant to the independent variable (intercept)\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Step 3: Fit the linear regression model\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "# Step 4: Get the estimated coefficient for public repositories\n",
        "additional_followers_per_repo = model.params[1]  # Coefficient for public_repos\n",
        "print(f\"Estimated additional followers per additional public repository: {additional_followers_per_repo:.4f}\")\n",
        "\n",
        "# Optional: Print the model summary for more insights\n",
        "print(model.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lt4u_jVTx24W"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# File paths for repositories.csv\n",
        "input_csv_path = \"/content/repositories.csv\"\n",
        "output_csv_path = \"/content/repositories.csv\"\n",
        "\n",
        "# Initialize a list to store processed repository data\n",
        "processed_repos = []\n",
        "\n",
        "# Step 1: Read and process repositories.csv\n",
        "with open(input_csv_path, mode=\"r\", encoding=\"utf-8\") as repos_file:\n",
        "    repos_reader = csv.DictReader(repos_file)\n",
        "\n",
        "    for repo in repos_reader:\n",
        "        # Convert empty or None values in has_project and has_wiki to \"Null\"\n",
        "        repo[\"has_projects\"] = repo[\"has_projects\"] if repo[\"has_projects\"] else \"Null\"\n",
        "        repo[\"has_wiki\"] = repo[\"has_wiki\"] if repo[\"has_wiki\"] else \"Null\"\n",
        "\n",
        "        # Append the processed repo data to the list\n",
        "        processed_repos.append(repo)\n",
        "\n",
        "# Step 2: Write the processed data back to a new CSV file\n",
        "with open(output_csv_path, mode=\"w\", newline='', encoding=\"utf-8\") as output_file:\n",
        "    # Use the fieldnames from the original file to maintain the same structure\n",
        "    fieldnames = processed_repos[0].keys()\n",
        "    writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
        "\n",
        "    # Write the header and data\n",
        "    writer.writeheader()\n",
        "    writer.writerows(processed_repos)\n",
        "\n",
        "print(f\"Processed data with 'Null' values saved to {output_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Gn4zfRHO3Qj"
      },
      "outputs": [],
      "source": [
        "#Correlation between number of followers and number of repositories.\n",
        "import csv\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# File path for repositories.csv\n",
        "repos_csv_path = \"/content/repositories.csv\"\n",
        "\n",
        "# Initialize lists to store has_projects and has_wiki flags\n",
        "has_projects = []\n",
        "has_wiki = []\n",
        "\n",
        "# Step 1: Read repositories from repositories.csv and extract has_projects and has_wiki\n",
        "with open(repos_csv_path, mode=\"r\", encoding=\"utf-8\") as repos_file:\n",
        "    repos_reader = csv.DictReader(repos_file)\n",
        "    count = 0\n",
        "    for repo in repos_reader:\n",
        "        count+=1\n",
        "        # Convert the string to a boolean\n",
        "        projects_enabled = repo.get(\"has_projects\") == 'True' # True if the string is \"True\"\n",
        "        # print(type(repo.get(\"has_projects\")))\n",
        "        # print(projects_enabled)\n",
        "        wiki_enabled = repo.get(\"has_wiki\") == 'True'  # True if the string is \"True\"\n",
        "\n",
        "        # projects_enabled_n = repo.get(\"has_projects\") == 'False'  # True if the string is \"True\"\n",
        "        # wiki_enabled_n = repo.get(\"has_wiki\") == 'False'  # True if the string is \"True\"\n",
        "\n",
        "        has_projects.append(1 if projects_enabled else 0)  # 1 for True, 0 for False\n",
        "        has_wiki.append(1 if wiki_enabled else 0)  # 1 for True, 0 for False\n",
        "    print(count)\n",
        "    print(has_projects.count(1))\n",
        "    print(has_wiki.count(1))\n",
        "    print(len(has_wiki))\n",
        "# Step 2: Calculate the correlation between has_projects and has_wiki\n",
        "if len(has_projects) > 1 and len(has_wiki) > 1:  # Ensure there is enough data\n",
        "    correlation,_ = pearsonr(has_projects, has_wiki)\n",
        "    print(f\"The correlation between enabling projects and having wikis enabled is: {correlation:.4f}\")\n",
        "else:\n",
        "    print(\"Not enough data to calculate correlation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKbGYShnPakb"
      },
      "outputs": [],
      "source": [
        "#Difference in Average hireable and Non-hireable.\n",
        "import csv\n",
        "\n",
        "# File path for users.csv\n",
        "users_csv_path = \"/content/users.csv\"\n",
        "\n",
        "# Initialize lists to store following counts for hireable and non-hireable users\n",
        "hireable_following = []\n",
        "non_hireable_following = []\n",
        "\n",
        "# Step 1: Read users from users.csv and separate based on hireable status\n",
        "with open(users_csv_path, mode=\"r\", encoding=\"utf-8\") as users_file:\n",
        "    users_reader = csv.DictReader(users_file)\n",
        "\n",
        "    for user in users_reader:\n",
        "        following_count = int(user.get(\"following\", 0))\n",
        "        hireable_status = user.get(\"hireable\") == 'true' # Convert to boolean\n",
        "\n",
        "        if hireable_status:\n",
        "            hireable_following.append(following_count)\n",
        "        else:\n",
        "            non_hireable_following.append(following_count)\n",
        "\n",
        "# Step 2: Calculate the average number of people followed for each group\n",
        "average_hireable = (sum(hireable_following) / len(hireable_following)) if hireable_following else 0\n",
        "average_non_hireable = (sum(non_hireable_following) / len(non_hireable_following)) if non_hireable_following else 0\n",
        "\n",
        "# Step 3: Calculate the difference between the averages\n",
        "difference = average_hireable - average_non_hireable\n",
        "\n",
        "# Step 4: Print the results\n",
        "print(f\"Average following for hireable users: {average_hireable:.3f}\")\n",
        "print(f\"Average following for non-hireable users: {average_non_hireable:.3f}\")\n",
        "print(f\"Difference in average following (hireable - non-hireable): {difference:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jRAYEP7hYR9"
      },
      "outputs": [],
      "source": [
        "#Finding most common surname in the data.\n",
        "import csv\n",
        "from collections import Counter\n",
        "\n",
        "# File path for users.csv\n",
        "users_csv_path = \"/content/users.csv\"\n",
        "\n",
        "# Initialize a list to store surnames\n",
        "surnames = []\n",
        "\n",
        "# Step 1: Read users from users.csv and extract the last word in each name as the surname\n",
        "with open(users_csv_path, mode=\"r\", encoding=\"utf-8\") as users_file:\n",
        "    users_reader = csv.DictReader(users_file)\n",
        "\n",
        "    for user in users_reader:\n",
        "        name = user.get(\"name\")\n",
        "\n",
        "        # Check if name exists and is not empty\n",
        "        if name:\n",
        "            # Split the name by whitespace and take the last word as the surname\n",
        "            surname = name.strip().split()[-1]\n",
        "            surnames.append(surname)\n",
        "\n",
        "# Step 2: Count occurrences of each surname\n",
        "surname_counts = Counter(surnames)\n",
        "\n",
        "# Step 3: Find the most common surname(s)\n",
        "if surname_counts:\n",
        "    max_count = max(surname_counts.values())\n",
        "    most_common_surnames = sorted([surname for surname, count in surname_counts.items() if count == max_count])\n",
        "    print(f\"The most common surname(s): {', '.join(most_common_surnames)}\")\n",
        "else:\n",
        "    print(\"No surnames found in the data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrozDxXD2Wus"
      },
      "outputs": [],
      "source": [
        "#Regression slope on word count of bio\n",
        "import csv\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# File path for users.csv\n",
        "users_csv_path = \"/content/users.csv\"\n",
        "\n",
        "# Initialize lists to store bio lengths and follower counts\n",
        "bio_lengths = []\n",
        "followers = []\n",
        "\n",
        "# Step 1: Read users.csv, filter users with bios, and calculate bio length\n",
        "with open(users_csv_path, mode=\"r\", encoding=\"utf-8\") as users_file:\n",
        "    users_reader = csv.DictReader(users_file)\n",
        "\n",
        "    for user in users_reader:\n",
        "        bio = user.get(\"bio\")\n",
        "        follower_count = int(user.get(\"followers\", 0))\n",
        "\n",
        "        # Ignore users without a bio\n",
        "        if bio:\n",
        "            # Count the number of words in the bio (split by whitespace)\n",
        "            bio_length = len(bio.split())\n",
        "            bio_lengths.append(bio_length)\n",
        "            followers.append(follower_count)\n",
        "\n",
        "# Step 2: Perform Linear Regression\n",
        "# Convert lists to NumPy arrays for regression analysis\n",
        "bio_lengths = np.array(bio_lengths).reshape(-1, 1)\n",
        "followers = np.array(followers)\n",
        "\n",
        "# Initialize and fit the regression model\n",
        "reg_model = LinearRegression()\n",
        "reg_model.fit(bio_lengths, followers)\n",
        "\n",
        "# Retrieve the slope (impact per word)\n",
        "slope = reg_model.coef_[0]\n",
        "\n",
        "# Output the slope rounded to 3 decimal places\n",
        "print(f\"Regression slope of followers on bio word count: {slope:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coHK6REl3AL9"
      },
      "outputs": [],
      "source": [
        "#Top 5 users who created repos on weekend.\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "# File path for repositories.csv\n",
        "repos_csv_path = \"/content/repositories.csv\"\n",
        "\n",
        "# Dictionary to count weekend repositories for each user\n",
        "weekend_repos_count = {}\n",
        "\n",
        "# Step 1: Read repositories.csv and count weekend repositories per user\n",
        "with open(repos_csv_path, mode=\"r\", encoding=\"utf-8\") as repos_file:\n",
        "    repos_reader = csv.DictReader(repos_file)\n",
        "\n",
        "    for repo in repos_reader:\n",
        "        login = repo.get(\"login\")\n",
        "        created_at = repo.get(\"created_at\")\n",
        "\n",
        "        # Parse the created_at date\n",
        "        created_date = datetime.strptime(created_at, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "\n",
        "        # Check if the day is a weekend (Saturday=5, Sunday=6)\n",
        "        if created_date.weekday() >= 5:\n",
        "            if login in weekend_repos_count:\n",
        "                weekend_repos_count[login] += 1\n",
        "            else:\n",
        "                weekend_repos_count[login] = 1\n",
        "\n",
        "# Step 2: Identify the top 5 users with the most weekend repositories\n",
        "top_5_users = sorted(weekend_repos_count, key=weekend_repos_count.get, reverse=True)[:5]\n",
        "\n",
        "# Output the top 5 users, comma-separated\n",
        "print(\"Top 5 users who created the most repositories on weekends (UTC):\", \", \".join(top_5_users))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx7sb3SRCowA"
      },
      "outputs": [],
      "source": [
        "#Difference in the fraction of hirable and nonhireable (hireable people who share their emails.)\n",
        "import csv\n",
        "\n",
        "# File path for users.csv\n",
        "users_csv_path = \"/content/users.csv\"\n",
        "\n",
        "# Counters for hireable and non-hireable users with and without emails\n",
        "hireable_with_email = 0\n",
        "hireable_total = 0\n",
        "non_hireable_with_email = 0\n",
        "non_hireable_total = 0\n",
        "\n",
        "# Step 1: Read users.csv and calculate counts\n",
        "with open(users_csv_path, mode=\"r\", encoding=\"utf-8\") as users_file:\n",
        "    users_reader = csv.DictReader(users_file)\n",
        "\n",
        "    for user in users_reader:\n",
        "        email = user.get(\"email\")\n",
        "        hireable = user.get(\"hireable\").lower() == \"true\"  # Convert to boolean\n",
        "\n",
        "        if hireable:\n",
        "            hireable_total += 1\n",
        "            if email:\n",
        "                hireable_with_email += 1\n",
        "        else:\n",
        "            non_hireable_total += 1\n",
        "            if email:\n",
        "                non_hireable_with_email += 1\n",
        "\n",
        "# Step 2: Calculate fractions and difference\n",
        "hireable_fraction = hireable_with_email / hireable_total if hireable_total > 0 else 0\n",
        "non_hireable_fraction = non_hireable_with_email / non_hireable_total if non_hireable_total > 0 else 0\n",
        "fraction_difference = hireable_fraction - non_hireable_fraction\n",
        "\n",
        "# Output the difference rounded to 3 decimal places\n",
        "print(f\"Difference in fraction: {fraction_difference:.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
